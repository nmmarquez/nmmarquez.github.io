<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>nmmarquez.github.io/</title>
   
   <link>http://nmmarquez.github.io/</link>
   <description>Demography, Data Science, and other Deliberations</description>
   <language>en-uk</language>
   <managingEditor> Neal Marquez</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>NB1 and NB2 Dispersion</title>
	  <link>//nb1-nb2-uncertainty</link>
	  <author>Neal Marquez</author>
	  <pubDate>2016-12-03T02:50:00-08:00</pubDate>
	  <guid>//nb1-nb2-uncertainty</guid>
	  <description><![CDATA[
	     <p>The other day I started working with a colleague on propagating uncertainty for
a negative binomial generalized linear model. To my surprise I found that there
was a couple of different ways to define the probability distribution that led
to more than a bit of confusion on my end, but got me messing around with how
you could simulate over-dispersed count data in a couple of ways. It also got me
working with TMB again and the woes of dealing with nonlinear optimization.</p>

<p>The first model is a mixture distribution model which incorporates a poisson
distribution but also a gamma distribution to account for the extra variance.
from what I have seen this is referred to as an NB1 model and looks like this.</p>

<script type="math/tex; mode=display">
y \sim Poisson(Gamma(\mu / \alpha, \alpha))
</script>

<p>Our distribution has a mean of <script type="math/tex">\mu</script> but that persists through both
distributions but the variance is greater than just a regular poisson
distribution because of the randomness introduced by the Gamma distribution
which itself has a variance of <script type="math/tex">\mu * \alpha</script>. Getting this to work for a
linear model with a log link required me to introduce a new random
variable <script type="math/tex">\nu</script> which would account for the over-dispersion and a full log link
model follows the form</p>

<script type="math/tex; mode=display">
\nu \sim gamma(1/\alpha, \alpha)
</script>

<script type="math/tex; mode=display">
\mu = exp(X \dot \beta + \nu)
</script>

<script type="math/tex; mode=display">
y \sim Poisson(\mu)
</script>

<p>where <script type="math/tex">X</script> is my matrix of covariate data and <script type="math/tex">\beta</script> are their coefficients
for the linear model.</p>

<p>Simulating this data was cake but when I used TMB to fit the model I found
that my model often failed to converge depending on what the start values of my
parameters were. I could get around it by fitting the model with out <script type="math/tex">\nu</script>
first to get good start values for <script type="math/tex">\beta</script> but even so the model was slow to
fit.</p>

<p>Alternatively we could model the extra variance directly without any extra term
$$\nu$ by using a negative binomial function that comes built in with both R and
TMB. This way saves us the hassle of having to simulate from two different
distributions and the two parameter model can easily be set up so that it has
a mean setting parameter and a variance parameter. The NB2 as well call it looks
like this.</p>

<script type="math/tex; mode=display">
y \sim NB2(\mu, \theta)
</script>

<script type="math/tex; mode=display">
E(y) = \mu
</script>

<script type="math/tex; mode=display">
Var(y) = \mu + (\theta / \mu^{2})
</script>

<p>Fitting a log linear model into here just means estimating <script type="math/tex">log(\mu)</script> with
the same <script type="math/tex">X \dot \beta</script> without the <script type="math/tex">\nu</script> parameter. The nice thing is we
still get the same distribution as the first model where <script type="math/tex">\alpha</script> corresponds
to <script type="math/tex">1/\theta</script>. The even nicer thing is that I didn’t have any problems fitting
this model and it was way faster for TMB due to not having a whole other set of
parameters to estimate.</p>

<p>A working version of the code exists <a href="https://github.com/nmmarquez/re_simulations/blob/master/nb_compare/nb_tmb.R">here</a> </p>

	  ]]></description>
	</item>

	<item>
	  <title>Simulating Spatially Correlated Data</title>
	  <link>//simulating-spatial-data</link>
	  <author>Neal Marquez</author>
	  <pubDate>2016-03-19T14:47:00-07:00</pubDate>
	  <guid>//simulating-spatial-data</guid>
	  <description><![CDATA[
	     <p>A couple of weeks ago I ran into a problem where I needed to create some spatially correlated data for a project and I
realized that I didn’t know the first thing about how to go about doing it. I was lucky to find that the
<a href="http://arxiv.org/abs/1601.01180">paper</a> that I was trying to replicate did a great job of detailing their methods in
such a way that I was able to create some spatial data of my own. The idea is that you need to create a covariance
matrix of the relationships of your data to pull form a multivariate random normal distribution.</p>

<script type="math/tex; mode=display">
\mathcal{N}(\mu, \Sigma)
</script>

<p><script type="math/tex">\mu</script> is just the scalar mean and can be set to any real number but defining a sensible covariance matrix was
originally difficult. The way that Riebler et al. did it was by using a matrix Q</p>

<script type="math/tex; mode=display">% <![CDATA[

Q_{i,j}=
\begin{cases}
    n_{\delta_{i}},& \text{if  } i = j \\
    -1,              & \text{if  } i \sim j \\
    0, & \text{otherwise}

\end{cases}
 %]]></script>

<p>where <script type="math/tex">\delta_{i}</script> is the number of neighbors a location has and <script type="math/tex">i \sim j</script> means that the two locations are
adjacent. Im used to working with spatial polygons data frames in R and found that its pretty easy to create this
matrix with the right packages. Here’s an example with US state shape file.</p>

<pre><code>library(sp)
library(surveillance)
library(spdep)
library(MASS)

# load in some USA data to work with
load(url("http://biogeo.ucdavis.edu/data/gadm2/R/USA_adm2.RData"))
cont_usa_locs &lt;- c("Texas", "Louisiana")

# spatial polygons data frame to simulate from
cont_usa &lt;- gadm[(gadm@data$NAME_1 %in% cont_usa_locs),]
cont_usa$ID &lt;- 1:length(cont_usa)

# adjacency matrix for spatial data 1's if adjacent 0 otherwise
adjmat &lt;- poly2adjmat(cont_usa)

# Create Q from an adjacency matrix
n_delta_i &lt;- rowSums(adjmat)
Q &lt;- adjmat * -1
diag(Q) &lt;- n_delta_i
</code></pre>

<p>Using the inverse of Q as the covariance matrix creates a smooth spatial dispersion, that you can see below, when
compared to just simple dispersion at random. A more complete example can be found
<a href="https://github.com/nmmarquez/spatial_epi/blob/master/project/project_outlay.r">here</a>.</p>

<p><img src="/assets/images/spatial_sim_plot.png" alt="Alt text" title="If it fits I sits." /></p>

	  ]]></description>
	</item>

	<item>
	  <title>Latex with Jekyll</title>
	  <link>//latex-with-jekyll</link>
	  <author>Neal Marquez</author>
	  <pubDate>2016-03-18T03:18:00-07:00</pubDate>
	  <guid>//latex-with-jekyll</guid>
	  <description><![CDATA[
	     <p>I am trying out a new layout that will hopefully let me do inline latex like so <script type="math/tex">\beta</script> as well as its own thing like  </p>

<script type="math/tex; mode=display">
Y = \beta_0 + x * \beta_1 
</script>

	  ]]></description>
	</item>

	<item>
	  <title>Hello Worlds</title>
	  <link>//hello-worlds</link>
	  <author>Neal Marquez</author>
	  <pubDate>2016-03-18T03:18:00-07:00</pubDate>
	  <guid>//hello-worlds</guid>
	  <description><![CDATA[
	     <p>Setting up a blog so I have a place to post my ideas and work that I manage to do outside of work and school (ha!).
Hoping this is the beginning of something long and beautiful so here goes nothing.</p>

	  ]]></description>
	</item>


</channel>
</rss>
